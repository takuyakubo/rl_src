"""
MDPï¼ˆãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼‰ã®å®Ÿè¨¼

Zennè¨˜äº‹ã§èª¬æ˜ã—ãŸMDPã®æ¦‚å¿µã‚’å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ã§ä½“é¨“ã™ã‚‹ã€‚
GridWorldEnvironmentã‚’ä½¿ã£ã¦ãƒãƒ«ã‚³ãƒ•æ€§ã¨æ™‚ä¸å¤‰æ€§ã‚’ç¢ºèªã™ã‚‹ã€‚
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import numpy as np
import matplotlib.pyplot as plt
from gridworld import GridWorldEnvironment


def main():
    """MDPå®Ÿè¨¼ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("=" * 60)
    print("MDPï¼ˆãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼‰ã®å®Ÿè¨¼ãƒ—ãƒ­ã‚°ãƒ©ãƒ ")
    print("=" * 60)
    
    # 1. æ±ºå®šçš„ç’°å¢ƒã§ã®MDPå®Ÿè¨¼
    demonstrate_deterministic_mdp()
    
    print("\n" + "=" * 60)
    
    # 2. ç¢ºç‡çš„ç’°å¢ƒã§ã®MDPå®Ÿè¨¼
    demonstrate_stochastic_mdp()
    
    print("\n" + "=" * 60)
    
    # 3. ç’°å¢ƒã®æ¯”è¼ƒå®Ÿé¨“
    compare_environments()
    
    print("\n" + "=" * 60)
    
    # 4. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œä¾‹
    run_sample_episodes()


def demonstrate_deterministic_mdp():
    """æ±ºå®šçš„MDPã®å®Ÿè¨¼"""
    print("\n=== æ±ºå®šçš„MDPç’°å¢ƒ ===")
    
    env = GridWorldEnvironment(size=3, stochastic=False, random_seed=42)
    
    # åŸºæœ¬æƒ…å ±ã‚’è¡¨ç¤º
    print(f"çŠ¶æ…‹ç©ºé–“ã®ã‚µã‚¤ã‚º: {len(env._state_space)}")
    print(f"è¡Œå‹•ç©ºé–“: {env.get_action_space()}")
    print(f"ã‚´ãƒ¼ãƒ«ä½ç½®: {env.goal}")
    
    # ãƒãƒ«ã‚³ãƒ•æ€§ã®å®Ÿè¨¼
    print("\n--- ãƒãƒ«ã‚³ãƒ•æ€§ã®å®Ÿè¨¼ ---")
    print("ç¾åœ¨ã®çŠ¶æ…‹ã®ã¿ãŒæ¬¡ã®çŠ¶æ…‹ã‚’æ±ºå®šã—ã¾ã™ã€‚")
    print("åŒã˜çŠ¶æ…‹ãƒ»è¡Œå‹•ãƒšã‚¢ã¯å¸¸ã«åŒã˜é·ç§»åˆ†å¸ƒã‚’æŒã¡ã¾ã™ã€‚")
    
    # æ™‚ä¸å¤‰æ€§ã®å®Ÿè¨¼
    print("\n--- æ™‚ä¸å¤‰æ€§ã®å®Ÿè¨¼ ---")
    print("é·ç§»ç¢ºç‡ã¯æ™‚åˆ»ã«ä¾å­˜ã—ã¾ã›ã‚“ã€‚")
    print("æ™‚åˆ»tã§ã®é·ç§»ã¨æ™‚åˆ»t+1ã§ã®é·ç§»ã¯åŒã˜ç¢ºç‡åˆ†å¸ƒã‚’æŒã¡ã¾ã™ã€‚")
    
    # å®Ÿéš›ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œ
    print("\n--- ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œä¾‹ ---")
    env.reset()
    print(f"åˆæœŸçŠ¶æ…‹: {env.get_current_state()}")
    
    actions = ["right", "right", "down", "down"]
    for i, action in enumerate(actions):
        if not env.is_done():
            next_state, reward, done, info = env.step(action)
            print(f"ã‚¹ãƒ†ãƒƒãƒ—{i+1}: {action} â†’ {next_state}, å ±é…¬={reward:.1f}, çµ‚äº†={done}")
        
        if env.is_done():
            break
    
    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ†æ
    history = env.get_history()
    total_reward = sum(transition.reward for transition in history)
    reached_goal = env.get_current_state() == env.goal
    
    print(f"\nã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµæœ:")
    print(f"  ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {len(history)}")
    print(f"  ç·å ±é…¬: {total_reward:.1f}")
    print(f"  ã‚´ãƒ¼ãƒ«åˆ°é”: {reached_goal}")


def demonstrate_stochastic_mdp():
    """ç¢ºç‡çš„MDPã®å®Ÿè¨¼"""
    print("\n=== ç¢ºç‡çš„MDPç’°å¢ƒ ===")
    
    env = GridWorldEnvironment(size=3, stochastic=True, random_seed=42)
    
    # ãƒãƒ«ã‚³ãƒ•æ€§ã®å®Ÿè¨¼ï¼ˆç¢ºç‡çš„ç’°å¢ƒã§ã‚‚åŒã˜ï¼‰
    print("\n--- ãƒãƒ«ã‚³ãƒ•æ€§ã®å®Ÿè¨¼ ---")
    print("ç¢ºç‡çš„ç’°å¢ƒã§ã‚‚ç¾åœ¨ã®çŠ¶æ…‹ã®ã¿ãŒæ¬¡ã®çŠ¶æ…‹ã‚’æ±ºå®šã—ã¾ã™ã€‚")
    
    # ç¢ºç‡åˆ†å¸ƒã®è©³ç´°ç¢ºèª
    print("\n--- ç¢ºç‡çš„é·ç§»ã®è©³ç´° ---")
    test_state = (1, 1)
    test_action = "right"
    
    print(f"çŠ¶æ…‹ {test_state} ã§è¡Œå‹• '{test_action}' ã‚’å–ã£ãŸå ´åˆ:")
    if env.stochastic:
        print(f"  â†’ æ„å›³ã—ãŸæ–¹å‘: 80%")
        print(f"  â†’ ä»–ã®æ–¹å‘: å„5%")
    else:
        print(f"  â†’ æ±ºå®šçš„ãªé·ç§»")
    
    # è¤‡æ•°å›å®Ÿè¡Œã—ã¦ç¢ºç‡çš„æ€§è³ªã‚’ç¢ºèª
    print("\n--- ç¢ºç‡çš„å®Ÿè¡Œã®ä¾‹ ---")
    results = []
    for episode in range(5):
        env.reset(start_position=test_state)
        next_state, reward, done, info = env.step(test_action)
        results.append(next_state)
        print(f"å®Ÿè¡Œ{episode+1}: {test_state} --{test_action}--> {next_state}")
    
    # çµæœã®åˆ†å¸ƒã‚’ç¢ºèª
    unique_results = set(results)
    print(f"\nè¦³æ¸¬ã•ã‚ŒãŸæ¬¡çŠ¶æ…‹: {unique_results}")
    if len(unique_results) > 1:
        print("â†’ ç¢ºç‡çš„ç’°å¢ƒã§ã¯ç•°ãªã‚‹çµæœãŒè¦³æ¸¬ã•ã‚Œã‚‹")
    else:
        print("â†’ ã“ã®å®Ÿè¡Œã§ã¯åŒã˜çµæœï¼ˆç¢ºç‡çš„ã§ã‚‚èµ·ã“ã‚Šã†ã‚‹ï¼‰")


def compare_environments():
    """ç’°å¢ƒã®æ¯”è¼ƒå®Ÿé¨“"""
    print("\n=== ç’°å¢ƒã®æ¯”è¼ƒå®Ÿé¨“ ===")
    
    # ç•°ãªã‚‹è¨­å®šã®ç’°å¢ƒã‚’ä½œæˆ
    environments = {
        "æ±ºå®šçš„ç’°å¢ƒ": GridWorldEnvironment(size=3, stochastic=False, random_seed=42),
        "ç¢ºç‡çš„ç’°å¢ƒ": GridWorldEnvironment(size=3, stochastic=True, random_seed=42),
        "å¤§ããªã‚³ã‚¹ãƒˆ": GridWorldEnvironment(size=3, stochastic=False, move_cost=-0.5, random_seed=42),
        "å¤§ããªå ±é…¬": GridWorldEnvironment(size=3, stochastic=False, goal_reward=10.0, random_seed=42)
    }
    
    test_trajectory = ["right", "down", "right", "down"]
    
    print("åŒã˜è»Œè·¡ã§ã®å„ç’°å¢ƒã®æ¯”è¼ƒ:")
    print("è»Œè·¡:", " â†’ ".join(test_trajectory))
    print()
    
    for env_name, env in environments.items():
        env.reset()
        total_reward = 0
        
        print(f"{env_name}:")
        for action in test_trajectory:
            if not env.is_done():
                next_state, reward, done, info = env.step(action)
                total_reward += reward
                print(f"  {action}: å ±é…¬={reward:.1f}")
                
                if done:
                    print(f"  â†’ ã‚´ãƒ¼ãƒ«åˆ°é”ï¼")
                    break
        
        print(f"  ç·å ±é…¬: {total_reward:.1f}")
        print()


def run_sample_episodes():
    """ã‚µãƒ³ãƒ—ãƒ«ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å®Ÿè¡Œ"""
    print("\n=== ã‚µãƒ³ãƒ—ãƒ«ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å®Ÿè¡Œ ===")
    
    env = GridWorldEnvironment(size=4, stochastic=False, random_seed=42)
    
    # ã‚ˆã‚Šå¤§ããªã‚°ãƒªãƒƒãƒ‰ã§ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œ
    print("4x4ã‚°ãƒªãƒƒãƒ‰ã§ã®ãƒ©ãƒ³ãƒ€ãƒ æ–¹ç­–ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰:")
    
    np.random.seed(42)
    
    for episode in range(3):
        print(f"\n--- ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {episode + 1} ---")
        
        env.reset()
        step_count = 0
        max_steps = 20
        
        print(f"é–‹å§‹ä½ç½®: {env.get_current_state()}")
        
        while not env.is_done() and step_count < max_steps:
            # ãƒ©ãƒ³ãƒ€ãƒ ã«è¡Œå‹•ã‚’é¸æŠ
            action = np.random.choice(env.get_action_space())
            next_state, reward, done, info = env.step(action)
            
            step_count += 1
            print(f"ã‚¹ãƒ†ãƒƒãƒ—{step_count}: {action} â†’ {next_state}, å ±é…¬={reward:.1f}")
            
            if done:
                print("ğŸ‰ ã‚´ãƒ¼ãƒ«åˆ°é”ï¼")
                break
        
        if not env.is_done():
            print("â° æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°ã«åˆ°é”")
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ†æ
        history = env.get_history()
        total_reward = sum(transition.reward for transition in history)
        print(f"çµæœ: {len(history)}ã‚¹ãƒ†ãƒƒãƒ—, ç·å ±é…¬={total_reward:.1f}")


def visualize_environment():
    """ç’°å¢ƒã®å¯è¦–åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"""
    print("\n=== ç’°å¢ƒã®å¯è¦–åŒ– ===")
    
    env = GridWorldEnvironment(size=4, stochastic=False)
    env.reset()
    
    # ã„ãã¤ã‹ã®è¡Œå‹•ã‚’å®Ÿè¡Œ
    actions = ["right", "right", "down", "left", "down", "right"]
    
    for action in actions:
        if not env.is_done():
            env.step(action)
    
    try:
        # ç°¡å˜ãªãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º
        print("ç¾åœ¨ã®ä½ç½®ã¨ã‚´ãƒ¼ãƒ«ã®é–¢ä¿‚:")
        current_pos = env.get_current_state()
        print(f"ç¾åœ¨ä½ç½®: {current_pos}")
        print(f"ã‚´ãƒ¼ãƒ«ä½ç½®: {env.goal}")
        print(f"è·é›¢: {abs(current_pos[0] - env.goal[0]) + abs(current_pos[1] - env.goal[1])}")
        
    except Exception as e:
        print(f"å¯è¦–åŒ–ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


if __name__ == "__main__":
    main()
    
    # å¯è¦–åŒ–ã‚‚å®Ÿè¡Œï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    try:
        visualize_environment()
    except ImportError:
        print("\nmatplotlib ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€å¯è¦–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"\nå¯è¦–åŒ–ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    print("\n" + "=" * 60)
    print("MDPå®Ÿè¨¼ãƒ—ãƒ­ã‚°ãƒ©ãƒ å®Œäº†")
    print("è©³ç´°ã¯ test_gridworld.py ã§ãƒ†ã‚¹ãƒˆã§ãã¾ã™")
    print("=" * 60)
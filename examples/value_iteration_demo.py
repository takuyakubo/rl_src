"""
ä¾¡å€¤åå¾©æ³•ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

MDPCoreä¸­å¿ƒã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ValueIterationStrategyã‚’ä½¿ç”¨ã—ã¦
ä¾¡å€¤åå¾©æ³•ã«ã‚ˆã‚‹æœ€é©æ–¹ç­–ã®è¨ˆç®—éç¨‹ã‚’å®Ÿè¨¼ã™ã‚‹ã€‚
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, Dict

from gridworld import GridWorldEnvironment
from gridworld_factory import create_gridworld_mdp_core
from src.agents.strategies.value_iteration import ValueIterationStrategy


def main():
    """ä¾¡å€¤åå¾©æ³•ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("=" * 70)
    print("ä¾¡å€¤åå¾©æ³•ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("=" * 70)
    
    # 1. åŸºæœ¬çš„ãªä¾¡å€¤åå¾©æ³•ã®å‹•ä½œç¢ºèª
    demonstrate_basic_value_iteration()
    
    print("\n" + "=" * 70)
    
    # 2. ç•°ãªã‚‹è¨­å®šã§ã®æ¯”è¼ƒ
    demonstrate_parameter_comparison()
    
    print("\n" + "=" * 70)
    
    # 3. ç¢ºç‡çš„ç’°å¢ƒã§ã®å‹•ä½œç¢ºèª
    demonstrate_stochastic_environment()
    
    print("\n" + "=" * 70)
    
    # 4. æ–¹ç­–ã®å®Ÿæ¼”
    demonstrate_policy_execution()


def demonstrate_basic_value_iteration():
    """åŸºæœ¬çš„ãªä¾¡å€¤åå¾©æ³•ã®å‹•ä½œç¢ºèª"""
    print("\n=== åŸºæœ¬çš„ãªä¾¡å€¤åå¾©æ³• ===")
    
    # 3x3ã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ã®MDPCoreã‚’ä½œæˆ
    mdp_core = create_gridworld_mdp_core(
        size=3, 
        stochastic=False, 
        goal=(2, 2),
        move_cost=-0.1,
        goal_reward=1.0,
        random_seed=42
    )
    
    # ç’°å¢ƒã‚’ä½œæˆ
    env = GridWorldEnvironment(mdp_core=mdp_core, goal=(2, 2), random_seed=42)
    
    print(f"ç’°å¢ƒè¨­å®š:")
    print(f"  ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º: {env.size}x{env.size}")
    print(f"  ã‚´ãƒ¼ãƒ«ä½ç½®: {env.goal}")
    print(f"  çŠ¶æ…‹æ•°: {len(mdp_core.states)}")
    print(f"  è¡Œå‹•æ•°: {len(mdp_core.actions)}")
    
    # ä¾¡å€¤åå¾©æ³•æˆ¦ç•¥ã‚’ä½œæˆ
    strategy = ValueIterationStrategy(
        mdp_core=mdp_core,
        gamma=0.9,
        theta=1e-6,
        max_iterations=1000
    )
    
    print(f"\nä¾¡å€¤åå¾©æ³•ã®è¨­å®š:")
    print(f"  å‰²å¼•ç‡: {strategy.gamma}")
    print(f"  åæŸé–¾å€¤: {strategy.theta}")
    print(f"  æœ€å¤§åå¾©å›æ•°: {strategy.max_iterations}")
    
    # ä¾¡å€¤åå¾©æ³•ã‚’å®Ÿè¡Œï¼ˆplanãƒ¡ã‚½ãƒƒãƒ‰ã§æœ€é©æ–¹ç­–ã‚’è¨ˆç®—ï¼‰
    print(f"\nä¾¡å€¤åå¾©æ³•ã‚’å®Ÿè¡Œä¸­...")
    optimal_policy = strategy.plan()
    
    # çµæœã‚’è¡¨ç¤º
    print(f"\nå­¦ç¿’çµæœ:")
    print(f"  è¨ˆç®—ã•ã‚ŒãŸæ–¹ç­–ã®ã‚µã‚¤ã‚º: {len(optimal_policy)}")
    
    # ä¾¡å€¤é–¢æ•°ã‚’è¡¨ç¤º
    print_value_function(strategy, env.size)
    
    # æœ€é©æ–¹ç­–ã‚’è¡¨ç¤º
    print_optimal_policy(strategy, env.size)
    
    # æ–¹ç­–ã®æ€§èƒ½ã‚’ãƒ†ã‚¹ãƒˆ
    test_policy_performance(strategy, env)


def demonstrate_parameter_comparison():
    """ç•°ãªã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šã§ã®æ¯”è¼ƒ"""
    print("\n=== ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šã®æ¯”è¼ƒ ===")
    
    # ç•°ãªã‚‹å‰²å¼•ç‡ã§ã®æ¯”è¼ƒ
    gamma_values = [0.5, 0.9, 0.99]
    
    for gamma in gamma_values:
        print(f"\n--- å‰²å¼•ç‡ Î³ = {gamma} ---")
        
        mdp_core = create_gridworld_mdp_core(size=3, stochastic=False, random_seed=42)
        
        strategy = ValueIterationStrategy(
            mdp_core=mdp_core,
            gamma=gamma,
            theta=1e-6,
            max_iterations=1000
        )
        
        optimal_policy = strategy.plan()
        
        # ã‚´ãƒ¼ãƒ«çŠ¶æ…‹ã®ä¾¡å€¤ã‚’è¡¨ç¤º
        goal_state = (2, 2)
        goal_value = strategy.V.get(goal_state, 0.0)
        print(f"ã‚´ãƒ¼ãƒ«çŠ¶æ…‹ {goal_state} ã®ä¾¡å€¤: {goal_value:.3f}")
        
        # é–‹å§‹çŠ¶æ…‹ã®ä¾¡å€¤ã‚’è¡¨ç¤º
        start_state = (0, 0)
        start_value = strategy.V.get(start_state, 0.0)
        print(f"é–‹å§‹çŠ¶æ…‹ {start_state} ã®ä¾¡å€¤: {start_value:.3f}")


def demonstrate_stochastic_environment():
    """ç¢ºç‡çš„ç’°å¢ƒã§ã®ä¾¡å€¤åå¾©æ³•"""
    print("\n=== ç¢ºç‡çš„ç’°å¢ƒã§ã®ä¾¡å€¤åå¾©æ³• ===")
    
    # ç¢ºç‡çš„ã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ã®MDPCoreã‚’ä½œæˆ
    mdp_core_stoch = create_gridworld_mdp_core(
        size=3, 
        stochastic=True, 
        random_seed=42
    )
    
    env_stoch = GridWorldEnvironment(mdp_core=mdp_core_stoch, goal=(2, 2), random_seed=42)
    
    print(f"ç¢ºç‡çš„ç’°å¢ƒè¨­å®š:")
    print(f"  æ„å›³ã—ãŸæ–¹å‘: 80%")
    print(f"  ä»–ã®æ–¹å‘: å„5%")
    
    strategy_stoch = ValueIterationStrategy(
        mdp_core=mdp_core_stoch,
        gamma=0.9,
        theta=1e-6,
        max_iterations=1000
    )
    
    print(f"\nä¾¡å€¤åå¾©æ³•ã‚’å®Ÿè¡Œä¸­...")
    optimal_policy_stoch = strategy_stoch.plan()
    
    # ä¾¡å€¤é–¢æ•°ã‚’è¡¨ç¤º
    print_value_function(strategy_stoch, env_stoch.size)
    
    # æ±ºå®šçš„ç’°å¢ƒã¨æ¯”è¼ƒ
    print(f"\n--- æ±ºå®šçš„ç’°å¢ƒã¨ã®æ¯”è¼ƒ ---")
    
    mdp_core_det = create_gridworld_mdp_core(size=3, stochastic=False, random_seed=42)
    strategy_det = ValueIterationStrategy(
        mdp_core=mdp_core_det,
        gamma=0.9,
        theta=1e-6,
        max_iterations=1000
    )
    strategy_det.plan()
    
    # é–‹å§‹çŠ¶æ…‹ã®ä¾¡å€¤ã‚’æ¯”è¼ƒ
    start_state = (0, 0)
    stochastic_value = strategy_stoch.V.get(start_state, 0.0)
    deterministic_value = strategy_det.V.get(start_state, 0.0)
    
    print(f"é–‹å§‹çŠ¶æ…‹ {start_state} ã®ä¾¡å€¤:")
    print(f"  ç¢ºç‡çš„ç’°å¢ƒ: {stochastic_value:.3f}")
    print(f"  æ±ºå®šçš„ç’°å¢ƒ: {deterministic_value:.3f}")
    print(f"  å·®åˆ†: {abs(stochastic_value - deterministic_value):.3f}")


def demonstrate_policy_execution():
    """å­¦ç¿’ã•ã‚ŒãŸæ–¹ç­–ã®å®Ÿè¡Œãƒ‡ãƒ¢"""
    print("\n=== å­¦ç¿’ã•ã‚ŒãŸæ–¹ç­–ã®å®Ÿè¡Œãƒ‡ãƒ¢ ===")
    
    # MDPCoreã¨ç’°å¢ƒã‚’ä½œæˆ
    mdp_core = create_gridworld_mdp_core(size=4, goal=(3, 3), stochastic=False, random_seed=42)
    env = GridWorldEnvironment(mdp_core=mdp_core, goal=(3, 3), random_seed=42)
    
    # ä¾¡å€¤åå¾©æ³•ã§æœ€é©æ–¹ç­–ã‚’å­¦ç¿’
    strategy = ValueIterationStrategy(
        mdp_core=mdp_core,
        gamma=0.9,
        theta=1e-6
    )
    strategy.plan()
    
    print(f"4x4ã‚°ãƒªãƒƒãƒ‰ã§ã®æœ€é©æ–¹ç­–å®Ÿè¡Œ:")
    print(f"ã‚´ãƒ¼ãƒ«: {env.goal}")
    
    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ
    current_state = env.reset()
    print(f"\né–‹å§‹çŠ¶æ…‹: {current_state}")
    
    step = 0
    max_steps = 20
    total_reward = 0
    
    while not env.is_done() and step < max_steps:
        # ç¾åœ¨ã®çŠ¶æ…‹ã§ã®è¡Œå‹•åˆ†å¸ƒã‚’å–å¾—
        available_actions = env.get_available_actions()
        action_probs = strategy.get_policy(current_state, available_actions)
        
        # æœ€é©è¡Œå‹•ã‚’é¸æŠï¼ˆæ±ºå®šçš„ãªã®ã§ç¢ºç‡1.0ã®è¡Œå‹•ï¼‰
        action = max(action_probs.keys(), key=lambda a: action_probs[a])
        
        print(f"ã‚¹ãƒ†ãƒƒãƒ—{step + 1}: çŠ¶æ…‹{current_state} â†’ è¡Œå‹•'{action}' (ç¢ºç‡: {action_probs[action]:.2f})")
        
        # è¡Œå‹•ã‚’å®Ÿè¡Œ
        next_state, reward, done, info = env.step(action)
        total_reward += reward
        step += 1
        
        print(f"        â†’ æ¬¡çŠ¶æ…‹{next_state}, å ±é…¬{reward:.2f}")
        
        current_state = next_state
        
        if done:
            print(f"ğŸ‰ ã‚´ãƒ¼ãƒ«åˆ°é”ï¼")
            break
    
    print(f"\nã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµæœ:")
    print(f"  ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {step}")
    print(f"  ç·å ±é…¬: {total_reward:.3f}")
    print(f"  æˆåŠŸ: {'ã¯ã„' if env.is_done() else 'ã„ã„ãˆ'}")


def print_value_function(strategy: ValueIterationStrategy, grid_size: int):
    """ä¾¡å€¤é–¢æ•°ã‚’ã‚°ãƒªãƒƒãƒ‰å½¢å¼ã§è¡¨ç¤º"""
    print(f"\nä¾¡å€¤é–¢æ•° (V*):")
    
    # ã‚°ãƒªãƒƒãƒ‰å½¢å¼ã§è¡¨ç¤º
    for i in range(grid_size):
        row = []
        for j in range(grid_size):
            value = strategy.V.get((i, j), 0.0)
            row.append(f"{value:6.3f}")
        print(f"  {' '.join(row)}")


def print_optimal_policy(strategy: ValueIterationStrategy, grid_size: int):
    """æœ€é©æ–¹ç­–ã‚’ã‚°ãƒªãƒƒãƒ‰å½¢å¼ã§è¡¨ç¤º"""
    print(f"\næœ€é©æ–¹ç­– (Ï€*):")
    
    # è¡Œå‹•ã®çŸ¢å°è¡¨ç¾
    action_arrows = {
        "up": "â†‘",
        "right": "â†’", 
        "down": "â†“",
        "left": "â†"
    }
    
    # ã‚°ãƒªãƒƒãƒ‰å½¢å¼ã§è¡¨ç¤º
    for i in range(grid_size):
        row = []
        for j in range(grid_size):
            action = strategy.policy.get((i, j), None)
            arrow = action_arrows.get(action, "?")
            row.append(f"  {arrow}  ")
        print(f"  {''.join(row)}")


def test_policy_performance(strategy: ValueIterationStrategy, env: GridWorldEnvironment):
    """å­¦ç¿’ã•ã‚ŒãŸæ–¹ç­–ã®æ€§èƒ½ã‚’ãƒ†ã‚¹ãƒˆ"""
    print(f"\n=== æ–¹ç­–ã®æ€§èƒ½ãƒ†ã‚¹ãƒˆ ===")
    
    # è¤‡æ•°ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§æ€§èƒ½ã‚’è©•ä¾¡
    num_episodes = 10
    episode_results = []
    
    for episode in range(num_episodes):
        current_state = env.reset()
        total_reward = 0
        steps = 0
        max_steps = 20
        
        while not env.is_done() and steps < max_steps:
            # æœ€é©è¡Œå‹•ã‚’å–å¾—
            available_actions = env.get_available_actions()
            action = strategy.get_action(current_state, available_actions)
            
            if action is None:
                print(f"è­¦å‘Š: çŠ¶æ…‹ {current_state} ã§è¡Œå‹•ãŒæ±ºå®šã§ãã¾ã›ã‚“ã§ã—ãŸ")
                break
            
            next_state, reward, done, info = env.step(action)
            total_reward += reward
            steps += 1
            current_state = next_state
        
        episode_results.append({
            "episode": episode + 1,
            "steps": steps,
            "total_reward": total_reward,
            "success": env.is_done()
        })
    
    # çµæœã‚’è¡¨ç¤º
    successful_episodes = [r for r in episode_results if r["success"]]
    success_rate = len(successful_episodes) / num_episodes
    
    print(f"ãƒ†ã‚¹ãƒˆçµæœ ({num_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰):")
    print(f"  æˆåŠŸç‡: {success_rate:.1%}")
    
    if successful_episodes:
        avg_steps = np.mean([r["steps"] for r in successful_episodes])
        avg_reward = np.mean([r["total_reward"] for r in successful_episodes])
        print(f"  å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ•°: {avg_steps:.1f}")
        print(f"  å¹³å‡ç·å ±é…¬: {avg_reward:.3f}")
    
    # è©³ç´°çµæœã‚’è¡¨ç¤ºï¼ˆæœ€åˆã®3ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰
    print(f"\nè©³ç´°çµæœ (æœ€åˆã®3ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰):")
    for result in episode_results[:3]:
        status = "æˆåŠŸ" if result["success"] else "å¤±æ•—"
        print(f"  ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰{result['episode']}: {result['steps']}ã‚¹ãƒ†ãƒƒãƒ—, "
              f"å ±é…¬{result['total_reward']:.3f}, {status}")


if __name__ == "__main__":
    main()
    
    print("\n" + "=" * 70)
    print("ä¾¡å€¤åå¾©æ³•ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†")
    print("=" * 70)
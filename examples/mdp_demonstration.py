"""
MDPï¼ˆãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼‰ã®å®Ÿè¨¼

Zennè¨˜äº‹ã§èª¬æ˜ã—ãŸMDPã®æ¦‚å¿µã‚’å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ã§ä½“é¨“ã™ã‚‹ã€‚
GridWorldEnvironmentã‚’ä½¿ã£ã¦ãƒãƒ«ã‚³ãƒ•æ€§ã¨æ™‚ä¸å¤‰æ€§ã‚’ç¢ºèªã™ã‚‹ã€‚
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import numpy as np
import matplotlib.pyplot as plt
from src.rl_src.environments import GridWorldEnvironment


def main():
    """MDPå®Ÿè¨¼ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("=" * 60)
    print("MDPï¼ˆãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼‰ã®å®Ÿè¨¼ãƒ—ãƒ­ã‚°ãƒ©ãƒ ")
    print("=" * 60)
    
    # 1. æ±ºå®šçš„ç’°å¢ƒã§ã®MDPå®Ÿè¨¼
    demonstrate_deterministic_mdp()
    
    print("\\n" + "=" * 60)
    
    # 2. ç¢ºç‡çš„ç’°å¢ƒã§ã®MDPå®Ÿè¨¼
    demonstrate_stochastic_mdp()
    
    print("\\n" + "=" * 60)
    
    # 3. ç’°å¢ƒã®æ¯”è¼ƒå®Ÿé¨“
    compare_environments()
    
    print("\\n" + "=" * 60)
    
    # 4. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œä¾‹
    run_sample_episodes()


def demonstrate_deterministic_mdp():
    """æ±ºå®šçš„MDPã®å®Ÿè¨¼"""
    print("\\n=== æ±ºå®šçš„MDPç’°å¢ƒ ===")
    
    env = GridWorldEnvironment(size=3, stochastic=False, random_seed=42)
    
    # åŸºæœ¬æƒ…å ±ã‚’è¡¨ç¤º
    print(f"çŠ¶æ…‹ç©ºé–“ã®ã‚µã‚¤ã‚º: {len(env.get_state_space())}")
    print(f"è¡Œå‹•ç©ºé–“: {env.get_action_space()}")
    print(f"ã‚´ãƒ¼ãƒ«ä½ç½®: {env.goal}")
    
    # ãƒãƒ«ã‚³ãƒ•æ€§ã®å®Ÿè¨¼
    env.demonstrate_markov_property()
    
    # æ™‚ä¸å¤‰æ€§ã®å®Ÿè¨¼
    env.demonstrate_time_invariance()
    
    # å®Ÿéš›ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œ
    print("\\n--- ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œä¾‹ ---")
    env.reset()
    print(f"åˆæœŸçŠ¶æ…‹: {env.get_current_state()}")
    
    actions = ["right", "right", "down", "down"]
    for i, action in enumerate(actions):
        if not env.is_done():
            next_state, reward, done, info = env.step(action)
            print(f"ã‚¹ãƒ†ãƒƒãƒ—{i+1}: {action} â†’ {next_state}, å ±é…¬={reward:.1f}, çµ‚äº†={done}")
        
        if env.is_done():
            break
    
    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ†æ
    analysis = env.analyze_episode()
    print(f"\\nã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµæœ:")
    print(f"  ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {analysis['total_steps']}")
    print(f"  ç·å ±é…¬: {analysis['total_reward']:.1f}")
    print(f"  ã‚´ãƒ¼ãƒ«åˆ°é”: {analysis['reached_goal']}")


def demonstrate_stochastic_mdp():
    """ç¢ºç‡çš„MDPã®å®Ÿè¨¼"""
    print("\\n=== ç¢ºç‡çš„MDPç’°å¢ƒ ===")
    
    env = GridWorldEnvironment(size=3, stochastic=True, random_seed=42)
    
    # ãƒãƒ«ã‚³ãƒ•æ€§ã®å®Ÿè¨¼ï¼ˆç¢ºç‡çš„ç’°å¢ƒã§ã‚‚åŒã˜ï¼‰
    env.demonstrate_markov_property()
    
    # ç¢ºç‡åˆ†å¸ƒã®è©³ç´°ç¢ºèª
    print("\\n--- ç¢ºç‡çš„é·ç§»ã®è©³ç´° ---")
    test_state = (1, 1)
    test_action = "right"
    
    transition_model = env.get_transition_model()
    prob_dist = transition_model[test_state][test_action]
    
    print(f"çŠ¶æ…‹ {test_state} ã§è¡Œå‹• '{test_action}' ã‚’å–ã£ãŸå ´åˆ:")
    for next_state, probability in prob_dist.items():
        if probability > 0:
            print(f"  â†’ {next_state}: {probability:.1f}")
    
    print(f"ç¢ºç‡ã®ç·å’Œ: {sum(prob_dist.values()):.1f}")
    
    # è¤‡æ•°å›å®Ÿè¡Œã—ã¦ç¢ºç‡çš„æ€§è³ªã‚’ç¢ºèª
    print("\\n--- ç¢ºç‡çš„å®Ÿè¡Œã®ä¾‹ ---")
    results = []
    for episode in range(5):
        env.reset(start_position=test_state)
        next_state, reward, done, info = env.step(test_action)
        results.append(next_state)
        print(f"å®Ÿè¡Œ{episode+1}: {test_state} --{test_action}--> {next_state}")
    
    # çµæœã®åˆ†å¸ƒã‚’ç¢ºèª
    unique_results = set(results)
    print(f"\\nè¦³æ¸¬ã•ã‚ŒãŸæ¬¡çŠ¶æ…‹: {unique_results}")
    if len(unique_results) > 1:
        print("â†’ ç¢ºç‡çš„ç’°å¢ƒã§ã¯ç•°ãªã‚‹çµæœãŒè¦³æ¸¬ã•ã‚Œã‚‹")
    else:
        print("â†’ ã“ã®å®Ÿè¡Œã§ã¯åŒã˜çµæœï¼ˆç¢ºç‡çš„ã§ã‚‚èµ·ã“ã‚Šã†ã‚‹ï¼‰")


def compare_environments():
    """ç’°å¢ƒã®æ¯”è¼ƒå®Ÿé¨“"""
    print("\\n=== ç’°å¢ƒã®æ¯”è¼ƒå®Ÿé¨“ ===")
    
    # ç•°ãªã‚‹è¨­å®šã®ç’°å¢ƒã‚’ä½œæˆ
    environments = {
        "æ±ºå®šçš„ç’°å¢ƒ": GridWorldEnvironment(size=3, stochastic=False, random_seed=42),
        "ç¢ºç‡çš„ç’°å¢ƒ": GridWorldEnvironment(size=3, stochastic=True, random_seed=42),
        "å¤§ããªã‚³ã‚¹ãƒˆ": GridWorldEnvironment(size=3, stochastic=False, move_cost=-0.5, random_seed=42),
        "å¤§ããªå ±é…¬": GridWorldEnvironment(size=3, stochastic=False, goal_reward=10.0, random_seed=42)
    }
    
    test_trajectory = ["right", "down", "right", "down"]
    
    print("åŒã˜è»Œè·¡ã§ã®å„ç’°å¢ƒã®æ¯”è¼ƒ:")
    print("è»Œè·¡:", " â†’ ".join(test_trajectory))
    print()
    
    for env_name, env in environments.items():
        env.reset()
        total_reward = 0
        
        print(f"{env_name}:")
        for action in test_trajectory:
            if not env.is_done():
                next_state, reward, done, info = env.step(action)
                total_reward += reward
                print(f"  {action}: å ±é…¬={reward:.1f}")
                
                if done:
                    print(f"  â†’ ã‚´ãƒ¼ãƒ«åˆ°é”ï¼")
                    break
        
        print(f"  ç·å ±é…¬: {total_reward:.1f}")
        print()


def run_sample_episodes():
    """ã‚µãƒ³ãƒ—ãƒ«ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å®Ÿè¡Œ"""
    print("\\n=== ã‚µãƒ³ãƒ—ãƒ«ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å®Ÿè¡Œ ===")
    
    env = GridWorldEnvironment(size=4, stochastic=False, random_seed=42)
    
    # ã‚ˆã‚Šå¤§ããªã‚°ãƒªãƒƒãƒ‰ã§ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œ
    print("4x4ã‚°ãƒªãƒƒãƒ‰ã§ã®ãƒ©ãƒ³ãƒ€ãƒ æ–¹ç­–ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰:")
    
    np.random.seed(42)
    
    for episode in range(3):
        print(f"\\n--- ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {episode + 1} ---")
        
        env.reset()
        step_count = 0
        max_steps = 20
        
        print(f"é–‹å§‹ä½ç½®: {env.get_current_state()}")
        
        while not env.is_done() and step_count < max_steps:
            # ãƒ©ãƒ³ãƒ€ãƒ ã«è¡Œå‹•ã‚’é¸æŠ
            action = np.random.choice(env.get_action_space())
            next_state, reward, done, info = env.step(action)
            
            step_count += 1
            print(f"ã‚¹ãƒ†ãƒƒãƒ—{step_count}: {action} â†’ {next_state}, å ±é…¬={reward:.1f}")
            
            if done:
                print("ğŸ‰ ã‚´ãƒ¼ãƒ«åˆ°é”ï¼")
                break
        
        if not env.is_done():
            print("â° æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°ã«åˆ°é”")
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ†æ
        analysis = env.analyze_episode()
        print(f"çµæœ: {analysis['total_steps']}ã‚¹ãƒ†ãƒƒãƒ—, ç·å ±é…¬={analysis['total_reward']:.1f}")


def visualize_environment():
    """ç’°å¢ƒã®å¯è¦–åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"""
    print("\\n=== ç’°å¢ƒã®å¯è¦–åŒ– ===")
    
    env = GridWorldEnvironment(size=4, stochastic=False)
    env.reset()
    
    # ã„ãã¤ã‹ã®è¡Œå‹•ã‚’å®Ÿè¡Œ
    actions = ["right", "right", "down", "left", "down", "right"]
    
    for action in actions:
        if not env.is_done():
            env.step(action)
    
    try:
        # RGBé…åˆ—ã¨ã—ã¦æç”»
        rgb_array = env.render(mode="rgb_array")
        print(f"ç’°å¢ƒã‚’å¯è¦–åŒ–ã—ã¾ã—ãŸï¼ˆé…åˆ—ã‚µã‚¤ã‚º: {rgb_array.shape}ï¼‰")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        plt.figure(figsize=(8, 8))
        plt.imshow(rgb_array)
        plt.title("GridWorld Environment")
        plt.axis('off')
        plt.savefig("gridworld_visualization.png", dpi=150, bbox_inches='tight')
        print("å¯è¦–åŒ–çµæœã‚’ 'gridworld_visualization.png' ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
    except Exception as e:
        print(f"å¯è¦–åŒ–ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("matplotlib ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")


if __name__ == "__main__":
    main()
    
    # å¯è¦–åŒ–ã‚‚å®Ÿè¡Œï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    try:
        visualize_environment()
    except ImportError:
        print("\\nmatplotlib ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€å¯è¦–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"\\nå¯è¦–åŒ–ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    print("\\n" + "=" * 60)
    print("MDPå®Ÿè¨¼ãƒ—ãƒ­ã‚°ãƒ©ãƒ å®Œäº†")
    print("è©³ç´°ã¯ test_gridworld.py ã§ãƒ†ã‚¹ãƒˆã§ãã¾ã™")
    print("=" * 60)